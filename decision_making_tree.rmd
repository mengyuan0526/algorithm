---
title: "Markov预测法在股票价格波动率预测中的应用"
author: "王梦圆"
date: "2020-02"
output:
  bookdown::html_document2:
    fig_caption: true
    highlight: haddock
    keep_md: true
    md_extensions: +east_asian_line_breaks
    number_sections: true
    pandoc_args:
    - --filter
    - pandoc-crossref
    - -M
    - eqnPrefix=
    seq_numbering: false
    toc: true
  bookdown::pdf_document2:
    keep_tex: true
    latex_engine: xelatex
    md_extensions: +east_asian_line_breaks
    pandoc_args:
    - --listing
    - --filter
    - pandoc-crossref
    toc: false
  slidy_presentation:
    highlight: haddock
  bookdown::word_document2:
    fig_caption: true
    md_extensions: +east_asian_line_breaks
    pandoc_args:
    - --filter
    - pandoc-crossref
    reference_docx: ./style/word-styles-02.docx
  ioslides_presentation:
    highlight: haddock
    slide_level: 3
  beamer_presentation:
    keep_tex: true
    latex_engine: xelatex
    toc: true
    pandoc_args:
    - --listing
    - --filter
    - pandoc-crossref
    slide_level: 3
    template: ./style/beamer-template.tex
csl: ./style/chinese-gb7714-2005-numeric.csl
css: ./style/markdown.css
bibliography: Bibfile.bib
eqnPrefixTemplate: ($$i$$)
institute: 中南财经政法大学统计与数学学院
link-citations: true
linkReferences: true
chapters: true
tableEqns: false
autoEqnLabels: false
---


# 有监督学习
## 决策树算法

1. ID3算法：在每个结点处选取能获得最高信息增益的分支属性进行分裂
2. 在每个决策结点处划分分支、选取分支属性的目的是将整个决策树的样本的纯度提升
3. 衡量样本集合纯度的指标则是熵
$$\text {Entropy}(S)=-\sum_{i=1}^{m} p_{i} \log _{2}\left(p_{i}\right), p_{i}=\frac{\left|C_{i}\right|}{n}$$

4. 信息增益
$$\operatorname{Gain}(S, \mathbb{A})=\operatorname{Entropy}(S)-\sum_{i=1}^{v}
\frac{\left|S_{i}\right|}{|S|} \operatorname{Entropy}\left(S_{i}\right)$$

缺点：同样区分度的两个属性，取值种类越多，信息增益也越多，

### C4.5 算法
总体思路与ID3类似，都是通过构造决策树进行分类，区别在于分类的处理，在分支的处
理，在分支属性的选取上，ID3算法使用信息增益作为度量，而C4.5算法引入了信息增益率
作为度量

$$\text { Gain-ratio(A) }=\frac{\operatorname{Gain}(A)}{-\sum_{i=1}^{v}
\frac{\left|S_{i}\right|}{|S|} \log _{2} \frac{\left|S_{i}\right|}{|S|}}$$

### C5.0 算法

1. C5.0 算法的目的是对含有大量数据的数据集进行分析
2. 与C4.5算法相比有以下优势：
   - 决策树构建时间要比C4.5算法快上数倍，同时生成的决策树规模也更小，拥有更少的叶子节点数
   - 使用了提升法（boosting),组合多个决策树来做分类，使准确率大大提高
   - 提升可选项由使用者视情况决定，例如是否考虑样本的权重、样本错误分类成本等

### CART算法

CART算法采用的是一种二分循环分割的方法，每次都把当前样本集划分头 两个子样本集，
使生成的决策树的结点均有两个分支，显然，这样就构造 了一个二叉树。如果分支属性有
多于两个取值，在分裂时会对属性值进行组合，选择最佳的两个组合分支。假设某属性存在
q个可能取值，那么以该属性作为分支属性，生成两个分支的分裂方法共有$2^(q-1)-1$种

CART算法在分支处理中分支属性的度量指标是Gini指标

$$\operatorname{Gini}(S)=1-\sum_{i=1}^{m} p_{i}^{2}, p_{i}=\frac{\left|C_{i}\right|}{|S|}$$

```{python}
import numpy as np
import random
from sklearn import tree
from graphviz import Source
np.random.seed(42)
X = np.random.randint(10,size=(100,4))
Y = np.random.randint(2,size=100)
a = np.column_stack((Y,X))
a
clf = tree.DecisionTreeClassifier(criterion="gini",max_depth=3)
clf = clf.fit(X,Y)
graph = Source(tree.export_graphviz(clf,out_file=None))
graph.format = 'png'
graph.render('cart_tree',view=True)

```

## 分类

分类是将事物按特性进行分类，例如将手写数字图片分类为对应的数字。

使用MNIST数据集
```{python}
#下载数据集
from sklearn.datasets import fetch_openml
mnist = fetch_openml("mnist_784")

X,y = mnist['data'],mnist['target']
X.shape
y.shape
#显示图像
import matplotlib
import matplotlib.pyplot as plt

some_digit = X[36000] #抓取任意一个X值
some_digit
some_digit_image = some_digit.reshape(28,28)#图片像素是28*28的数组，抓取一个实例的特征向量，重新形成一个28*28的数组

#用matplotlib的imshow（）函数将图显示出来
plt.imshow(some_digit_image,cmap=matplotlib.cm.binary,interpolation="nearest")

plt.axis("off")#关闭坐标轴
plt.show()

#上面的图像看起来是8，根据标签y来验证，是正确的
y[36000]#字符串
import numpy as np
y = y.astype(np.uint8)
y[36000]#数值型


```

划分训练集(前60000张)和测试集（后10000张图像）
```{python}
X_train,X_test,y_train,y_test = X[:60000],X[60000:],y[:60000],y[60000:]

```
对训练集数据要重新洗牌
```{python}
import numpy as np 
shuffle_index = np.random.permutation(60000)
X_train,y_train = X_train[shuffle_index],y_train[shuffle_index]
```

### 训练一个二元分类器
二元分类器（数字8检测器）只能区别两个类别：8和非8。

创建目标向量：

```{python}
y_train_9 = (y_train == 9) # True for all 5s, False for all other digits
y_test_9 = (y_test==9)

```
选择随机梯度下降（SGD）分类器，使用Slearn-Learn的SGDClassifier类即可。这个分类器的优势是，能够有效处理非常大型的数据集。此时先创建一个SGDClassifier并在整个训练集上进行训练：

```{python}
from sklearn.linear_model import SGDClassifier
sgd_clf =SGDClassifier(random_state=42)#random_state=42设置随机数种子，保证每次结果可以复现
sgd_clf.fit(X_train,y_train_9)
```



```{python}

sgd_clf.predict([some_digit])

```
输出结果是False，表示分类器划分错误。

### 性能考核

1. 使用交叉验证测量精度

利用cross_val_score()这一类交叉验证的函数

```{python}
from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone


skfold = StratifiedKFold(n_splits=3,random_state=42)

for train_index,test_index in skfold.split(X_train,y_train_9):
    clone_clf = clone(sgd_clf)
    X_train_folds = X_train[train_index]
    y_train_folds = (y_train_9[train_index])
    X_test_folds = X_train[test_index]
    y_test_folds = (y_train_9[test_index])

    clone_clf.fit(X_train_folds,y_train_folds)
    y_pred = clone_clf.predict(X_test_folds)
    n_correct = sum(y_pred == y_test_folds)
    print(n_correct/len(y_pred))


```
用cross_val_score（）函数来评估SGDClassifier模型，采用K-fold交叉验证法，3 个折叠。记住，K-fold交叉验证的意思是将训练集分解成K个折叠（在本例中，为3折），然后每次留其中1个折叠进行预测，剩余的折叠用来训练
```{python}
from sklearn.model_selection import cross_val_score
##得到每层的准确率
cross_val_score(sgd_clf,X_train,y_train_9,cv=3,scoring="accuracy")
```
准确率不是成为分类器的首要性能指标，特别是处理偏斜数据的时候

### 混淆矩阵

评估分类器性能的更好方法是混淆矩阵。总体思路就是统计A类别实例被分成为B类别的次数。
要计算混淆矩阵，需要先有一组预测才能将其与实际目标进行比较。作为替代，可以使用cross_val_predict（）函数：
```{python}
from sklearn.model_selection import cross_val_predict
y_train_pred = cross_val_predict(sgd_clf,X_train,y_train_9,cv=3)#返回的不是评估分数，而是每个折叠的预测

#获取混淆矩阵
from sklearn.metrics import confusion_matrix
confusion_matrix(y_train_9,y_train_pred)

```
得到的混淆矩阵
$$\left[\begin{array}{cc}
52096 & 1955 \\
1469 & 4480
\end{array}\right]$$
其中行表示实际类别，列表示预测类别。

计算精度和召回率

```{python}
from sklearn.metrics import precision_score,recall_score
precision_score(y_train_9,y_train_pred)#精度
recall_score(y_train_9,y_train_pred)#召回率

```

F1分数：由精度和召回率组成的一个单一指标
计算F1分数，只需要调用f1_score()即可

```{python}
from sklearn.metrics import f1_score
f1_score(y_train_9,y_train_pred)#计算f1分数

```

使用cross_val_predict（）函数获取训练集中所有实例的分数，但是这次需要它返回的是决策分数而不是预测结果,以决定使用什么阈值。

```{python}
y_scores = cross_val_predict(sgd_clf,X_train,y_train_9,cv=3,method="decision_function")

```
有了这些分数，可以使用precision_recall_curve（）函数来计算所有可能的阈值的精度和召回率：

```{python}
from sklearn.metrics import precision_recall_curve
precision,recalls,thresholds = precision_recall_curve(y_train_9,y_scores)

def plot_precision_recall_vs_threshold(precision,recalls,thresholds):
    plt.plot(thresholds,precision[:-1],'b--',label="Precision")
    plt.plot(thresholds,recalls[:-1],'g-',label="Recall")
    plt.xlabel("Threshold")
    plt.legend(loc="upper left")
    plt.ylim([0,1])

plot_precision_recall_vs_threshold(precision,recalls,thresholds)
plt.plot(recalls,precision)
plt.show()

```
## 训练模型
### 线性回归（Linear Regression）

普通函数
```{python}
import numpy as np
X = 2*np.random.rand(100,1)
y = 4+3*X + np.random.rand(100,1)
X_b = np.c_[np.ones((100,1)),X]
X_b
theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)
theta_best
X_new = np.array([[0],[2]])
X_new_b = np.c_[np.ones((2,1)),X_new]
y_predict = X_new_b.dot(theta_best)
y_predict

```

Scikit-Learn的等效代码

```{python}
from sklearn.linear_model import LinearRegression
fit1 = LinearRegression()
fit1.fit(X,y)
fit1.coef_,fit1.intercept_
fit1.predict(X_new)
```
### 梯度下降
梯度下降是一种非常通用的优化算法，能够为大范围的问题找到最优解。梯度下降的中心思想就是迭代地调整参数从而使成本函数最小化。


回归任务的典型性能衡量指标是均方根误差（RMSE），指的是预测过程中的标准偏差。
```{python eval=F,echo=F,error=T}
import os
import tarfile
from six.moves import urllib
DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml/master/"
HOUSING_PATH = "datasets/housing"
HOUSING_URL = DOWNLOAD_ROOT+HOUSING_PATH+"/housing.tgz"
HOUSING_URL

def fetch_housing_data(housing_url=HOUSING_URL,housing_path=HOUSING_PATH):
   #检查是否有这个路径，没有的话新创建一个
    if not os.path.isdir(housing_path):
        os.makedirs(housing_path)
        tgz_path = os.path.join(housing_path,"housing.tgz")#拼接路径
        urllib.request.urlretrieve(housing_url,"tgz_path")
        housing_tgz = tarfile.open(tgz_path)
        housing_tgz.extractall(path=housing_path)
        housing_tgz.close()


import pandas as pd 
def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path,"housing.csv")
    return pd.read_csv(csv_path)

housing = load_housing_data()


```

## K-近邻算法（k-Nearest Neighbors）

## 逻辑回归(Logistic Regrression)
## 支持向量机（Support Vector Machines,SVM）
## 决策树和随机森林（Decision Trees and Random Factor）
## 神经网络（Neural networks）


# 无监督式学习
## 聚类算法
## k-平均算法（k-means）
## 分层聚类分析（Hierarchiral Cluster Analysis）
## 最大期望算法（Expectation Maximization）
## 